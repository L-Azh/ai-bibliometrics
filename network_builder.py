"""
network_builder.py
ç½‘ç»œæ„å»ºæ¨¡å— - åˆä½œç½‘ç»œã€æœºæ„ç½‘ç»œã€å…³é”®è¯ç½‘ç»œ
"""

import networkx as nx
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import community as community_louvain
from typing import List
from cnki_parser import CnkiPaper


class NetworkBuilder:
    """ç½‘ç»œæ„å»ºå™¨"""
    
    def __init__(self, papers: List[CnkiPaper]):
        self.papers = papers
    
    def build_coauthorship(self, top_n=50):
        """ä½œè€…åˆä½œç½‘ç»œ"""
        G = nx.Graph()
        author_papers = defaultdict(list)
        
        # ç»Ÿè®¡æ¯ä½ä½œè€…çš„è®ºæ–‡
        for paper in self.papers:
            for author in paper.authors:
                if author:
                    author_papers[author].append(paper)
        
        # åªä¿ç•™é«˜äº§ä½œè€… Top N
        top_authors = sorted(author_papers.items(), 
                           key=lambda x: len(x[1]), 
                           reverse=True)[:top_n]
        top_names = {name for name, _ in top_authors}
        
        # æ„å»ºåˆä½œè¾¹
        for paper in self.papers:
            authors = [a for a in paper.authors if a in top_names]
            for i, a1 in enumerate(authors):
                for a2 in authors[i+1:]:
                    if G.has_edge(a1, a2):
                        G[a1][a2]['weight'] += 1
                    else:
                        G.add_edge(a1, a2, weight=1)
        
        # æ·»åŠ èŠ‚ç‚¹å±æ€§
        for node in G.nodes():
            G.nodes[node]['paper_count'] = len(author_papers[node])
            G.nodes[node]['citations'] = sum(p.citations for p in author_papers[node])
        
        print(f"âœ… åˆä½œç½‘ç»œ: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")
        return G
    
    def build_institution(self):
        """æœºæ„åˆä½œç½‘ç»œï¼ˆæ”¯æŒå¤šæœºæ„ %+ï¼‰"""
        G = nx.Graph()
        
        for paper in self.papers:
            institutions = paper.all_institutions
            if len(institutions) < 2:
                continue
            
            # ä¸ºæ‰€æœ‰æœºæ„å¯¹æ·»åŠ è¾¹
            for i, inst1 in enumerate(institutions):
                for inst2 in institutions[i+1:]:
                    if G.has_edge(inst1, inst2):
                        G[inst1][inst2]['weight'] += 1
                    else:
                        G.add_edge(inst1, inst2, weight=1)
        
        # æ·»åŠ èŠ‚ç‚¹å±æ€§
        for node in G.nodes():
            G.nodes[node]['paper_count'] = sum(
                1 for p in self.papers if node in p.all_institutions
            )
        
        print(f"âœ… æœºæ„ç½‘ç»œ: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")
        return G
    
    def build_keywords(self, min_freq=2):
        """å…³é”®è¯å…±ç°ç½‘ç»œ"""
        G = nx.Graph()
        
        # ç»Ÿè®¡è¯é¢‘
        keyword_freq = Counter()
        for paper in self.papers:
            for kw in paper.keywords:
                if kw:
                    keyword_freq[kw] += 1
        
        # åªä¿ç•™é«˜é¢‘è¯
        valid_keywords = {kw for kw, freq in keyword_freq.items() 
                         if freq >= min_freq}
        
        # ç»Ÿè®¡å…±ç°
        cooccurrence = Counter()
        for paper in self.papers:
            keywords = [kw for kw in paper.keywords if kw in valid_keywords]
            for i, kw1 in enumerate(keywords):
                for kw2 in keywords[i+1:]:
                    if kw1 != kw2:
                        pair = tuple(sorted([kw1, kw2]))
                        cooccurrence[pair] += 1
        
        # æ„å»ºç½‘ç»œï¼ˆåªä¿ç•™å…±ç°â‰¥2æ¬¡ï¼‰
        for (kw1, kw2), weight in cooccurrence.items():
            if weight >= 2:
                G.add_edge(kw1, kw2, weight=weight)
                G.nodes[kw1]['freq'] = keyword_freq[kw1]
                G.nodes[kw2]['freq'] = keyword_freq[kw2]
        
        print(f"âœ… å…³é”®è¯ç½‘ç»œ: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")
        return G


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    from cnki_parser import quick_parse
    
    print("ğŸ§ª æµ‹è¯•ç½‘ç»œæ„å»º...")
    papers, df, stats = quick_parse("test.enw")
    
    builder = NetworkBuilder(papers)
    
    # æ„å»ºä¸‰ç§ç½‘ç»œ
    coauth = builder.build_coauthorship(top_n=10)
    inst = builder.build_institution()
    keyword = builder.build_keywords(min_freq=1)
    
    print(f"\nğŸ“Š ç½‘ç»œç»Ÿè®¡:")
    print(f"  åˆä½œç½‘ç»œå¯†åº¦: {nx.density(coauth):.3f}")
    print(f"  æœºæ„ç½‘ç»œå¯†åº¦: {nx.density(inst):.3f}")
    print(f"  å…³é”®è¯ç½‘ç»œå¯†åº¦: {nx.density(keyword):.3f}")